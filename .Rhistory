knitr::opts_chunk$set(echo = TRUE)
bc<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",header=F,sep=",")
names(bc)<- c('id_number', 'diagnosis', 'radius_mean',
'texture_mean', 'perimeter_mean', 'area_mean',
'smoothness_mean', 'compactness_mean',
'concavity_mean','concave_points_mean',
'symmetry_mean', 'fractal_dimension_mean',
'radius_se', 'texture_se', 'perimeter_se',
'area_se', 'smoothness_se', 'compactness_se',
'concavity_se', 'concave_points_se',
'symmetry_se', 'fractal_dimension_se',
'radius_worst', 'texture_worst',
'perimeter_worst', 'area_worst',
'smoothness_worst', 'compactness_worst',
'concavity_worst', 'concave_points_worst',
'symmetry_worst', 'fractal_dimension_worst')
#Getting a look at the distribution of the response
table(bc$diagnosis)
#Train/Validation Split
library(caret)
set.seed(1234)
trainIndex<-createDataPartition(bc$diagnosis,p=.5,list=F)  #p: proportion of data in train
training<-bc[trainIndex,]
validate<-bc[-trainIndex,]
library(glmnet)
# Create a GLMNET model with alpha = 1 (Lasso) and lambda = 1
model <- glmnet(x = training, y = training$diagnosis, alpha = 1, lambda = 1)
View(training)
library(glmnet)
# Convert the response variable to a numeric variable
training$diagnosis <- as.numeric(training$diagnosis) - 1
# Create a GLMNET model with alpha = 1 (Lasso) and lambda = 1
model <- glmnet(x = training, y = training$diagnosis, alpha = 1, lambda = 1)
library(glmnet)
# Convert the response variable to a numeric variable
training$diagnosis <- as.numeric(training$diagnosis) - 1
# Impute the missing values
training <- missForest(training, maxiter = 5)$imputed
library(glmnet)
library(missForest)
# Convert the response variable to a numeric variable
training$diagnosis <- as.numeric(training$diagnosis) - 1
# Impute the missing values
training <- missForest(training, maxiter = 5)$imputed
# Create a GLMNET model with alpha = 1 (Lasso) and lambda = 1
model <- glmnet(x = training, y = training$diagnosis, alpha = 1, lambda = 1)
library(glmnet)
library(missForest)
# Convert the response variable to a numeric variable
training$diagnosis <- as.numeric(training$diagnosis) - 1
# Impute the missing values
training <- missForest(training, maxiter = 5)$imputed
View(training)
bc<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",header=F,sep=",")
names(bc)<- c('id_number', 'diagnosis', 'radius_mean',
'texture_mean', 'perimeter_mean', 'area_mean',
'smoothness_mean', 'compactness_mean',
'concavity_mean','concave_points_mean',
'symmetry_mean', 'fractal_dimension_mean',
'radius_se', 'texture_se', 'perimeter_se',
'area_se', 'smoothness_se', 'compactness_se',
'concavity_se', 'concave_points_se',
'symmetry_se', 'fractal_dimension_se',
'radius_worst', 'texture_worst',
'perimeter_worst', 'area_worst',
'smoothness_worst', 'compactness_worst',
'concavity_worst', 'concave_points_worst',
'symmetry_worst', 'fractal_dimension_worst')
#Getting a look at the distribution of the response
table(bc$diagnosis)
#Train/Validation Split
library(caret)
set.seed(1234)
trainIndex<-createDataPartition(bc$diagnosis,p=.5,list=F)  #p: proportion of data in train
training<-bc[trainIndex,]
validate<-bc[-trainIndex,]
library(glmnet)
library(missForest)
# Convert the response variable to a numeric variable
training$diagnosis <- as.numeric(training$diagnosis) - 1
# Impute the missing values
training <- missForest(training, maxiter = 5)$imputed
# Create a GLMNET model with alpha = 1 (Lasso) and lambda = 1
model <- glmnet(x = training, y = training$diagnosis, alpha = 1, lambda = 1)
library(glmnet)
library(caret)
fit <- cv.glmnet(x = training[, -1], y = training$diagnosis, alpha = 1, nfolds = 10, type.measure = "logloss")
fit <- cv.glmnet(x = training[, -1], y = training$diagnosis, alpha = 1, nfolds = 10, type.measure = "default")
library(glmnet)
library(caret)
fitControl<-trainControl(method="repeatedcv",number=5,repeats=1,classProbs=TRUE, summaryFunction=mnLogLoss)
set.seed(1234)
#GLMNET
glmnet.fit<-train(diagnosis~.,
data=training,
method="glmnet",
trControl=fitControl,
metric="logLoss")
bc<-read.table("https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data",header=F,sep=",")
names(bc)<- c('id_number', 'diagnosis', 'radius_mean',
'texture_mean', 'perimeter_mean', 'area_mean',
'smoothness_mean', 'compactness_mean',
'concavity_mean','concave_points_mean',
'symmetry_mean', 'fractal_dimension_mean',
'radius_se', 'texture_se', 'perimeter_se',
'area_se', 'smoothness_se', 'compactness_se',
'concavity_se', 'concave_points_se',
'symmetry_se', 'fractal_dimension_se',
'radius_worst', 'texture_worst',
'perimeter_worst', 'area_worst',
'smoothness_worst', 'compactness_worst',
'concavity_worst', 'concave_points_worst',
'symmetry_worst', 'fractal_dimension_worst')
#Getting a look at the distribution of the response
table(bc$diagnosis)
#Train/Validation Split
library(caret)
set.seed(1234)
trainIndex<-createDataPartition(bc$diagnosis,p=.5,list=F)  #p: proportion of data in train
training<-bc[trainIndex,]
validate<-bc[-trainIndex,]
library(glmnet)
library(caret)
fitControl<-trainControl(method="repeatedcv",number=5,repeats=1,classProbs=TRUE, summaryFunction=mnLogLoss)
set.seed(1234)
#GLMNET
glmnet.fit<-train(diagnosis~ ,
library(glmnet)
library(caret)
fitControl<-trainControl(method="repeatedcv",number=5,repeats=1,classProbs=TRUE, summaryFunction=mnLogLoss)
set.seed(1234)
#GLMNET
glmnet.fit<-train(diagnosis~ . ,
data=training,
method="glmnet",
trControl=fitControl,
metric="logLoss")
coef(glmnet.fit$finalModel,glmnet.fit$finalModel$lambdaOpt)
library(glmnet)
library(caret)
fitControl<-trainControl(method="repeatedcv",number=10,repeats=1,classProbs=TRUE, summaryFunction=mnLogLoss)
set.seed(1234)
#GLMNET
glmnet.fit<-train(diagnosis~ . ,
data=training,
method="glmnet",
trControl=fitControl,
metric="logLoss")
coef(glmnet.fit$finalModel,glmnet.fit$finalModel$lambdaOpt)
# libraries
library(glmnet)
library(caret)
library(pROC)
# knn model
knn.fit <- train(diagnosis ~ ., data = training, method = "knn", trControl = fitControl, metric = "logLoss")
# evaluate
logloss <- logLoss(predict(knn.fit, validate[, -1], type = "prob")[, 1], validate$diagnosis)
# evaluate
logLoss <- logLoss(predict(knn.fit, validate[, -1], type = "prob")[, 2], validate$diagnosis)
auc <- auc(predict(knn.fit, validate[, -1], type = "prob")[, 2], validate$diagnosis)
# libraries
library(glmnet)
library(caret)
library(pROC)
# knn model
knn.fit <- train(diagnosis ~ ., data = training, method = "knn", trControl = fitControl, metric = "logLoss")
# evaluate
logLoss <- logLoss(predict(knn.fit, validate[, -1], type = "prob")[, 2], validate$diagnosis)
roc.glmnet <- roc(validate$diagnosis, predict(glmnet.fit, validate[, -1], type = "prob")[, 1])
# libraries
library(glmnet)
library(caret)
library(pROC)
# knn model
knn.fit <- train(diagnosis ~ ., data = training, method = "knn", trControl = fitControl, metric = "logLoss")
# evaluate
logLoss <- logLoss(predict(knn.fit, validate[, -1], type = "prob")[, 2], validate$diagnosis)
auc <- auc(predict(knn.fit, validate[, -1], type = "prob")[, 2], validate$diagnosis)
# libraries
library(glmnet)
library(caret)
library(pROC)
# knn model
knn.fit <- train(diagnosis ~ ., data = training, method = "knn", trControl = fitControl, metric = "logLoss")
# Obtain the predicted probabilities for the KNN model
knn.probs <- predict(knn.fit, newdata = validate, type = "prob")
# Compute ROC curve for KNN model
knn.roc <- roc(validate$diagnosis, knn.probs[, "1"])
# libraries
library(glmnet)
library(caret)
library(pROC)
# knn model
knn.fit <- train(diagnosis ~ ., data = training, method = "knn", trControl = fitControl, metric = "logLoss")
# Obtain the predicted probabilities for the KNN model
knn.probs <- as.numeric(predict(knn.fit, newdata = validate, type = "prob")[, 2])
# Compute ROC curve for KNN model
knn.roc <- roc(validate$diagnosis, knn.probs)
# Obtain the predicted probabilities for the GLMNET model
glmnet.probs <- predict(glmnet.fit, newdata = validate, type = "response")
# libraries
library(glmnet)
library(caret)
library(pROC)
# knn model
knn.fit <- train(diagnosis ~ ., data = training, method = "knn", trControl = fitControl, metric = "logLoss")
# Obtain the predicted probabilities for the KNN model
knn.probs <- as.numeric(predict(knn.fit, newdata = validate, type = "prob")[, 2])
# Compute ROC curve for KNN model
knn.roc <- roc(validate$diagnosis, knn.probs)
# Obtain the predicted probabilities for the GLMNET model
glmnet.probs <- predict(glmnet.fit, newx = as.matrix(predictMatrix), s = "lambda.min")
# Compute ROC curve for GLMNET model
glmnet.roc <- roc(validate$diagnosis, glmnet.probs)
# Load necessary packages
library(caret)
library(pROC)
# Train KNN model with 10-fold cross-validation
knn.fit <- train(diagnosis ~ .,
data = training,
method = "knn",
trControl = fitControl,
metric = "logLoss")
# Obtain the predicted probabilities for the KNN model
knn.probs <- as.numeric(predict(knn.fit, newdata = validate, type = "prob")[, 2])
# Convert the response variable to numeric for ROC curve calculation
validate$diagnosis_numeric <- ifelse(validate$diagnosis == "M", 1, 0)
# Compute ROC curve for KNN model
knn.roc <- roc(validate$diagnosis_numeric, knn.probs)
# Obtain the predicted probabilities for the GLMNET model
glmnet.probs <- predict(glmnet.fit, newx = as.matrix(predictMatrix), s = "lambda.min")
# Compute ROC curve for GLMNET model
glmnet.roc <- roc(validate$diagnosis_numeric, glmnet.probs)
# Load necessary packages
library(caret)
library(pROC)
# Train KNN model with 10-fold cross-validation
knn.fit <- train(diagnosis ~ .,
data = training,
method = "knn",
trControl = fitControl,
metric = "logLoss")
# Obtain the predicted probabilities for the KNN model
knn.probs <- as.numeric(predict(knn.fit, newdata = validate, type = "prob")[, 2])
# Convert the response variable to numeric for ROC curve calculation
validate$diagnosis_numeric <- ifelse(validate$diagnosis == "M", 1, 0)
# Convert the GLMNET predicted probabilities to numeric
glmnet.probs <- as.numeric(predict(glmnet.fit, newx = as.matrix(validate[, -2]), s = "lambda.min"))
# Compute ROC curve for KNN model
knn.roc <- roc(validate$diagnosis_numeric, knn.probs)
# Compute ROC curve for GLMNET model
glmnet.roc <- roc(validate$diagnosis_numeric, glmnet.probs)
plot(knn.roc, col = "red", add = TRUE, lty = 2)
# Plot ROC curves
plot(glmnet.roc, col = "blue", main = "ROC Curve Comparison", lty = 1, xlim = c(0, 1), ylim = c(0, 1))
plot(knn.roc, col = "red", add = TRUE, lty = 2)
legend(0.6, 0.2, legend = c("GLMNET", "KNN"), col = c("blue", "red"), lty = 1:2, cex = 0.8)
# Calculate AUROC values
glmnet.auroc <- auc(glmnet.roc)
knn.auroc <- auc(knn.roc)
print(paste("GLMNET AUROC:", glmnet.auroc))
knitr::opts_chunk$set(echo = TRUE)
adult <- read.csv("E:/Docs/School/SMU-MSDS/Trimester 2/DS 6372 Applied Statistics/Unit 15 Project 2/adult/adult.data", header=FALSE)
library(dplyr)
# rename columns
adult <- rename(adult, age = V1)
adult <- rename(adult, workclass = V2)
adult <- rename(adult, fnlwgt = V3)
adult <- rename(adult, education = V4)
adult <- rename(adult, education_num = V5)
adult <- rename(adult, marital_status = V6)
adult <- rename(adult, occupation = V7)
adult <- rename(adult, relationship = V8)
adult <- rename(adult, race = V9)
adult <- rename(adult, sex = V10)
adult <- rename(adult, capital_gain = V11)
adult <- rename(adult, capital_loss = V12)
adult <- rename(adult, hours_per_week = V13)
adult <- rename(adult, native_country = V14)
adult <- rename(adult, class = V15)
table(adult$class)
# Convert the class variable to a factor
adult$class <- as.factor(adult$class)
# Create a new column called "class.num"
#adult$class.num <- ifelse(adult$class == "<=50K", 0, 1)
table(adult$class.num)
# Train/Validation Split
library(caret)
set.seed(4321)
trainIndex<-createDataPartition(adult$class,p=.7,list=F)  #p: proportion of data in train
training<-adult[trainIndex,]
validate<-adult[-trainIndex,]
library(MASS)
library(caret)
# LDA model
lda.model <- lda(class ~ age + education + marital_status + occupation + relationship + race + sex + hours_per_week + native_country, data = training)
# Predict the response variable for the test set
lda.predictions <- predict(lda.model, newdata = validate)$class
# Evaluate the model performance
cm <- table(lda.predictions, validate$class)
accuracy <- sum(diag(cm)) / sum(cm)
# Print the accuracy
print(accuracy)
#Calculate the confusion matrix
confusion_matrix = table(validate$class, lda.predictions)
#Calculate the sensitivity, specificity, prevalence, PPV, NPV, and AUROC
sensitivity = confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
specificity = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
prevalence = sum(confusion_matrix[1, ]) / nrow(validate)
ppv = confusion_matrix[1, 1] / sum(confusion_matrix[, 1])
npv = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
auc = pROC::auc(validate$class, lda.predictions)
library(MASS)
library(caret)
# LDA model
lda.model <- lda(class ~ age + education + marital_status + occupation + relationship + race + sex + hours_per_week + native_country, data = training)
# Predict the response variable for the test set
lda.predictions <- predict(lda.model, newdata = validate)$class
# Evaluate the model performance
cm <- table(lda.predictions, validate$class)
accuracy <- sum(diag(cm)) / sum(cm)
# Print the accuracy
print(accuracy)
#Calculate the confusion matrix
confusion_matrix = table(validate$class, lda.predictions)
#Calculate the sensitivity, specificity, prevalence, PPV, NPV, and AUROC
sensitivity = confusion_matrix[1, 1] / sum(confusion_matrix[1, ])
specificity = confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
prevalence = sum(confusion_matrix[1, ]) / nrow(validate)
ppv = confusion_matrix[1, 1] / sum(confusion_matrix[, 1])
npv = confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
#auc = pROC::auc(validate$class, lda.predictions)
#Print the results
print(paste("Sensitivity:", sensitivity))
print(paste("Specificity:", specificity))
print(paste("Prevalence:", prevalence))
print(paste("PPV:", ppv))
print(paste("NPV:", npv))
#print(paste("AUROC:", auc))
